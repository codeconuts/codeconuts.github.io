---
title: "Commutativity of Rotations"
date: 2024-03-24
draft: false
description: "Two Theorems Regarding Commutativity of Rotations"
tags: ["math"]
---
{{< katex >}}

# Prerequisites

## Cross Products Are Linear Maps

For a cross product \(\textbf{a}\times\textbf{b}\), we may consider it as an operation \(\textbf{a}_{\times}\) acting on \(\textbf{b}\). Note that the linearity of cross product

$$\textbf{a}_\times(\textbf{b}+\textbf{c})=\textbf{a}_\times\textbf{b}+\textbf{a}_\times\textbf{c}$$

implies that \(\textbf{a}_\times\) is a linear transformation. Therefore, we can represent \(\textbf{a}_\times\) with a \(3\times3\) matrix. One can work out the cross product to show that

$$\textbf{a}_\times=\begin{pmatrix}0&-a_z&a_y\\
a_z&0&-a_x\\
-a_y&a_x&0\end{pmatrix}$$

Note that \(\textbf{a}_\times\) is skew-symmetric, which makes it a generator of rotation.

## Axis-Angle Representation

By Euler's Rotation Theorem, any rotation in \(\mathbb{R}^{3}\) can be represented by

1. An axis of rotation \(\hat{\textbf{k}}\)
2. An angular displacement / angle or rotation \(\theta\) about \(\hat{\textbf{k}}\)

To preserve bijection between the rotation group and axis-angle representation, we define the *right-hand* rule, where

1. The thumb points in the direction of \(\hat{\textbf{k}}\)
2. All other fingers point in the direction of rotation

We can then define the axis-angle vector which represents a rotation
$$\boldsymbol{\theta}=\theta\hat{\textbf{k}}\qquad\theta\in(-\pi,\pi],\quad\hat{\textbf{k}}\in\{\textbf{v}\in\mathbb{R}_{\geq0}^3\mid |\textbf{v}|=1\}$$

## Rodrigues' Rotation Formula

In \(\mathbb{R}^3\), if \(v\) is rotated about \(\hat{\mathbf{k}}\) for an angle \(\theta\), the resulting vector is
$$\mathbf{v}_{\text {rot}} =\mathbf{v}+\sin (\theta) \hat{\mathbf{k}} \times \mathbf{v}+(1-\cos \theta) \hat{\mathbf{k}} \times(\hat{\mathbf{k}} \times \mathbf{v})$$

We can collect the cross product operations and define the rotation matrix for the aforementioned rotation

$$R=I+\sin(\theta)\hat{\mathbf{k}}_\times+(1-\cos\theta)\hat{\mathbf{k}}_\times^2$$

With the Taylor series of \(e^x\), we can verify that

$$R=\exp(\theta\hat{\mathbf{k}}_\times)$$

Hence, rotation generated by \(\theta\hat{\mathbf{k}}_\times\) is exactly the same as the rotation represented by the axis-angle vector \(\theta\hat{\mathbf{k}}\).

## Real Eigenvector and Axis of Rotation

The eigenspace with eigenvalue \(1\) is the axis of rotation.

Rotation matrices have orthogonal eigenspaces for distinct eigenvalues, and the eigenvalue \(1\) is not repeated (except for \(I\)), so it corresponds to a 1D eigenspace. Also, the axis-angle vector \(\theta\hat{\mathbf{k}}\) satisfies \(\exp(\theta\hat{\mathbf{k}}_\times)\theta\hat{\mathbf{k}}=\theta\hat{\mathbf{k}}\) can be easily verified using Rodrigues' Rotation Formula. Therefore, \(\theta\hat{\mathbf{k}}\) spans the 1D eigenspace with eigenvalue \(1\), making the eigenspace the axis of rotation.

# 1st Theorem

## Motivation

In spherical coordinates \((r,\theta,\phi)\), suppose we would like to move \((r,0,0)\) to \((r,\theta,\phi)\).

We can imagine first pulling the vector \(r\hat{\textbf{x}}\) up by angle \(\pi-\phi\), then rotating it on the \(x\)-\(y\) plane by angle \(\theta\). This is equivalent to first performing a rotation \(\boldsymbol{\phi}=(\pi-\phi)(-\hat{\mathbf{x}})\), then a rotation \(\boldsymbol{\theta}=\theta\hat{\mathbf{z}}\). However, we can also imagine first rotating the vector on the \(x\)-\(y\) plane by angle \(\theta\), then pulling it up by angle \(\pi-\phi\). This is equivalent to first performing a rotation \(\boldsymbol{\theta}=\theta\hat{\mathbf{z}}\), then a rotation \(\boldsymbol{\phi}=(\pi-\phi)(\sin(\theta)\hat{\mathbf{y}}-\cos(\theta)\hat{\mathbf{x}})\). Note that for the action of pulling the vector up by angle \(\pi-\phi\), its corresponding axis-angle vector \(\boldsymbol{\phi}\) has rotated too.

## Statement

Below, we prove in general, and coordinates-free, that the order of any two rotations can be swapped, if after swapping, we rotate the axis-angle vector of the 2nd rotation according to the 1st rotation as well.

Mathematically, denote the following

- \(A=\exp(\textbf{a}_\times)\) as the rotation matrix for the axis-angle vector \(\textbf{a}\)
- \(B=\exp(\textbf{b}_\times)\) as the rotation matrix for the axis-angle vector \(\textbf{b}\)

Additionally, we denote \(A'\) as the rotation matrix for the angular displacement vector \(\textbf{a}'\), where \(\textbf{a}'=B\textbf{a}\). Then, the theorem states that
$$BA=A'B$$

## Proof

Let \(\mathcal{B}\) be the orthonormal basis of the column vectors of the rotation matrix \(B\). Denote \([\textbf{v}]_\mathcal{B}=B^{-1}\textbf{v}\) the vector \(\textbf{v}\) in the basis \(\mathcal{B}\), and \([M]_\mathcal{B}=B^{-1}MB\) the matrix in the basis \(\mathcal{B}\).

We will show that for any \(\textbf{v}\in\mathbb{R}^3\),

$$\exp(([\textbf{v}]_\mathcal{B})_\times)=\exp([\textbf{v}_\times]_\mathcal{B})=[\exp(\textbf{v}_\times)]_\mathcal{B}$$

We will first show the left-side identity. For fixed vectors \(\textbf{v},\textbf{w}\) and any arbitrary vector \(\textbf{u}\), recall the volume of the parallelepiped formed by the 3 vectors, or alternatively by the scalar triple product, we have
$$\langle B\textbf{u},B\textbf{v}\times B\textbf{w}\rangle=\det\begin{pmatrix}\mid&\mid&\mid \\ B\textbf{u}&B\textbf{v}&B\textbf{w} \\ \mid&\mid&\mid\end{pmatrix}$$
Also, since \(B\) is a rotation matrix, which has properties \(\det(B)=1\) and \(\langle\textbf{p},\textbf{q}\rangle=\langle B\textbf{p},B\textbf{q}\rangle\),
$$\begin{align*}\det\begin{pmatrix}\mid&\mid&\mid \\ B\textbf{u}&B\textbf{v}&B\textbf{w} \\ \mid&\mid&\mid\end{pmatrix}&= \det(B)\det\begin{pmatrix}\mid&\mid&\mid \\ \textbf{u}&\textbf{v}&\textbf{w} \\ \mid&\mid&\mid\end{pmatrix}\\
&=\det(B)\langle \textbf{u},\textbf{v}\times \textbf{w}\rangle\\
&=\langle B\textbf{u},B(\textbf{v}\times \textbf{w})\rangle\end{align*}$$

Hence, for any arbitrary vector \(\textbf{u}\),
$$\langle B\textbf{u},B\textbf{v}\times B\textbf{w}\rangle=\langle B\textbf{u},B(\textbf{v}\times \textbf{w})\rangle$$
which implies
$$B\textbf{v}\times B\textbf{w}= B(\textbf{v}\times\textbf{w})$$
i.e. we can factor out the matrix \(B\). Then, for any vector \(\textbf{w}\),
$$\begin{align*}
B^{-1}\textbf{v}\times\textbf{w}&= B^{-1}(\textbf{v}\times B\textbf{w})\\
&= B^{-1}\textbf{v}_\times B\textbf{w}\\
([\textbf{v}]_\mathcal{B})_\times\textbf{w}&= [\textbf{v}_\times]_\mathcal{B}\textbf{w}
\end{align*}$$
This implies
$$([\textbf{v}]_\mathcal{B})_\times= [\textbf{v}_\times]_\mathcal{B}$$
which gives the left-side identity if we \(\exp\) both sides. The right-side identity is straightforward. By the Taylor series of \(e^x\), we get
$$\begin{align*}
\exp([\textbf{v}_\times]_\mathcal{B})&=I+[\textbf{v}_\times]_\mathcal{B}+ \frac{1}{2}([\textbf{v}_\times]_\mathcal{B})^{2}+\cdots\\
&=B^{-1}B+B^{-1}\textbf{v}_\times B+ \frac{1}{2}(B^{-1}\textbf{v}_\times B)^{2}+\cdots\\
&=B^{-1}B+B^{-1}\textbf{v}_\times B+ \frac{1}{2}B^{-1}\textbf{v}_\times^{2}B+\cdots\\
&=B^{-1}\exp(\textbf{v}_\times)B\\
&= [\exp(\textbf{v}_\times)]_\mathcal{B}
\end{align*}$$
Hence, we conclude with the identity
$$\exp(([\textbf{v}]_\mathcal{B})_\times)=[\exp(\textbf{v}_\times)]_\mathcal{B}$$

Substitute \(\textbf{v}=\textbf{a}'\) on both sides of the equation, we get
$$\begin{align*}
\exp((B^{-1}B\textbf{a})_\times)&=B^{-1}\exp({\textbf{a}'}_\times)B\\
BA&=A'B
\end{align*}$$

# 2nd Theorem

## Statement

Two rotations commute if and only if one of the following
- their axes of rotations are parallel
- at least one of the rotations is of angle \(0\)
- their axes of rotations are perpendicular and the angles of rotations are \(\pi\)

## Proof

Define two rotations \(R_1=\exp(\theta_1(\hat{\textbf{k}}_1)_\times),R_2=\exp(\theta_2(\hat{\textbf{k}}_2)_\times)\).

We first show that two commuting rotations imply one of the 3 conditions. Using the 1st theorem, we get \(R_2R_1=R_1'R_2\) and \(R_1R_2=R_2'R_1\). Assuming \(R_2R_1=R_1R_2\), then we get the system of equations

$$\begin{cases}R_1R_2=R_1'R_2\\R_2R_1=R_2'R_1\end{cases}\Longrightarrow\begin{cases}R_1=R_1'\\R_2=R_2'\end{cases}$$

We focus on the 1st equation. It can be written as

$$\exp(\theta_1(\hat{\textbf{k}}_1)_\times)=\exp(\theta_1'(\hat{\textbf{k}}_1')_\times)$$

where \(R_2(\theta_1\hat{\textbf{k}}_1)=\theta_1'\hat{\textbf{k}}_1'\). Note that \(\theta_1=0\) satisfies the equation with no restrictions on \(R_2\). We assume below that \(\theta_1\neq0\). Then,

$$\exp(\theta_1(\hat{\textbf{k}}_1)_\times)\hat{\textbf{k}}_1'=\exp(\theta_1'(\hat{\textbf{k}}_1')_\times)\hat{\textbf{k}}_1'=\hat{\textbf{k}}_1'$$

So \(\hat{\textbf{k}}_1'\) is in the eigenspace spanned by \(\hat{\textbf{k}}_1\). Both are unit vectors, and we restricted the unit vectors to the first quadrant, so \(\hat{\textbf{k}}_1'=\hat{\textbf{k}}_1\). Then,

$$R_2(\theta_1\hat{\textbf{k}}_1)=\theta_1'\hat{\textbf{k}}_1$$

This implies \(\hat{\textbf{k}}_1\) is an eigenvector of \(R_2\). Since \(\theta_1,\theta_1'\in\mathbb{R}\), \(\theta_1'/\theta_1=1\) which is the only real eigenvalue of \(R_2\) for general \(\theta_2\). This implies \(\hat{\textbf{k}}_1=\hat{\textbf{k}}_2\). An exception is when \(\theta_2=\pi\), it is also possible that \(\theta_1'/\theta_1=-1\), which is another real eigenvalue. Since this eigenvalue is different from that of the axis-angle vector, and rotation matrices have orthogonal eigenspaces, \(\hat{\textbf{k}}_1\cdot\hat{\textbf{k}}_2=0\).

We can use the same argument on the 2nd equation to obtain \(\hat{\textbf{k}}_1=\hat{\textbf{k}}_2\) for general \(\theta_1\), and an exception at \(\theta_1=\pi\) where \(\hat{\textbf{k}}_1\cdot\hat{\textbf{k}}_2=0\).

Now we show that any of the 3 conditions is sufficient for the two rotations to commute.

- If \(\hat{\textbf{k}}_1=\hat{\textbf{k}}_2\), we can apply the Baker–Campbell–Hausdorff formula to get
$$\begin{align*}\exp(\theta_1(\hat{\textbf{k}}_1)_\times)\exp(\theta_2(\hat{\textbf{k}}_1)_\times)&=\exp((\theta_1+\theta_2)(\hat{\textbf{k}}_1)_\times)\\&=\exp((\theta_2+\theta_1)(\hat{\textbf{k}}_1)_\times)\\&=\exp(\theta_2(\hat{\textbf{k}}_1)_\times)\exp(\theta_1(\hat{\textbf{k}}_1)_\times)\end{align*}$$

- A rotation of angle \(0\) implies \(R=I\), which commutes with any matrix.

- Plugging \(\theta=\pi\) into Rodrigues' Rotation Formula, we get
$$R=I+2\hat{\mathbf{k}}_\times^2$$
We compute the commutator
$$[R_1,R_2]=[I+2(\hat{\mathbf{k}}_1)_\times^2,I+2(\hat{\mathbf{k}}_2)_\times^2]=[2(\hat{\mathbf{k}}_1)_\times^2,2(\hat{\mathbf{k}}_2)_\times^2]$$
For any vector \(\textbf{v}\), given that \(\hat{\mathbf{k}}_1\cdot\hat{\mathbf{k}}_2=0\), and by applying the Vector Triple Product, we get (note that cross products are not associative)
$$\begin{align*}
&\hat{\mathbf{k}}_1\times\hat{\mathbf{k}}_1\times\hat{\mathbf{k}}_2\times\hat{\mathbf{k}}_2\times\textbf{v}\\&=\hat{\mathbf{k}}_1\times\hat{\mathbf{k}}_1\times((\hat{\mathbf{k}}_2\cdot\textbf{v})\hat{\mathbf{k}}_2-\textbf{v})\\
&=(\hat{\mathbf{k}}_2\cdot\textbf{v})((\hat{\mathbf{k}}_1\cdot\hat{\mathbf{k}}_2)\hat{\mathbf{k}}_1-\hat{\mathbf{k}}_2)-(\hat{\mathbf{k}}_1\cdot\textbf{v})\hat{\mathbf{k}}_1-\textbf{v}\\
&=-(\hat{\mathbf{k}}_2\cdot\textbf{v})\hat{\mathbf{k}}_2-(\hat{\mathbf{k}}_1\cdot\textbf{v})\hat{\mathbf{k}}_1-\textbf{v}
\end{align*}$$
Swapping indicies \(1\) and \(2\) give the same result. Hence, \([(\hat{\mathbf{k}}_1)_\times^2,(\hat{\mathbf{k}}_2)_\times^2]=0\) which implies \([R_1,R_2]=0\).